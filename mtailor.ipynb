{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Marupillasriram/scaffold/blob/main/mtailor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuwnqlAFVcCq",
        "outputId": "eb9d4c24-8685-4d2b-9627-8fbf07bc0c3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.10/dist-packages (0.0.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (4.9.2)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.13)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.11.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2022.12.7)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.65.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.26.15)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.4.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4)\n"
          ]
        }
      ],
      "source": [
        "! pip install bs4 lxml kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmWBfWYZcwy_",
        "outputId": "34fdd4ca-e602-4e99-b342-098e17651242"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (0.11.4)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.0.1+cu118)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJcxgV59cwvg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = 'bilalyousaf0014'\n",
        "os.environ['KAGGLE_KEY'] = '11031bc21c5e3ec23585dbe17dc4267d'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8501e0Fcwq_",
        "outputId": "9db0f7bf-cc88-403c-b36e-bfda2717884b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ml-engineer-assessment-dataset.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d bilalyousaf0014/ml-engineer-assessment-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoWwsuiscwoC",
        "outputId": "7e6cd364-31c4-4281-abd0-dee14d056819"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/ml-engineer-assessment-dataset.zip\n",
            "replace assessment_dataset/images/00001.jpeg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "! unzip /content/ml-engineer-assessment-dataset.zip\n",
        "     "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "created folders to save the output and required details."
      ],
      "metadata": {
        "id": "KOTuWS03pDu3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nkb7M4Ukzz2B",
        "outputId": "f037cbaf-9c3d-43a6-c32a-65f81293d429"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder '/content/output' already exists.\n",
            "Folder '/content/model' already exists.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "out = \"/content/output\"  # Replace with the desired folder path\n",
        "model = \"/content/model\"\n",
        "\n",
        "folder_path = [out, model]\n",
        "\n",
        "for path in folder_path:\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "        print(f\"Folder '{path}' created successfully.\")\n",
        "    else:\n",
        "        print(f\"Folder '{path}' already exists.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#importing required libraries\n"
      ],
      "metadata": {
        "id": "U8A7vCFDpNI7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWPYzVVlcwkm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "from torchvision.models import resnet18, resnet50, ResNet18_Weights, ResNet50_Weights\n",
        "import torchmetrics\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import pdb\n",
        "from PIL import ImageDraw\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from PIL import Image\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.init as init\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##listing species and animals to map them for further procrssing"
      ],
      "metadata": {
        "id": "MRpNYI6apVng"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACHkCRFGcwgr",
        "outputId": "8a8919d8-02eb-47e5-949a-e2472e38f6cc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'cat': 0, 'dog': 1, 'NA': 2},\n",
              " {'Abyssinian': 0,\n",
              "  'Birman': 1,\n",
              "  'Persian': 2,\n",
              "  'american_bulldog': 3,\n",
              "  'american_pit_bull_terrier': 4,\n",
              "  'basset_hound': 5,\n",
              "  'beagle': 6,\n",
              "  'chihuahua': 7,\n",
              "  'pomeranian': 8,\n",
              "  'NA': 9})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "SPECIES = ['Abyssinian', 'Birman', 'Persian', 'american_bulldog', 'american_pit_bull_terrier', 'basset_hound', 'beagle', 'chihuahua', 'pomeranian', 'NA']\n",
        "SPECIES_TO_INDEX = {specie: index for index, specie in enumerate(SPECIES)}\n",
        "\n",
        "\n",
        "\n",
        "CAT_OR_DOG = ['cat', 'dog', 'NA']\n",
        "CAT_OR_DOG_TO_INDEX = {cat_or_dog: index for index, cat_or_dog in enumerate(CAT_OR_DOG)}\n",
        "CAT_OR_DOG_TO_INDEX, SPECIES_TO_INDEX"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#intialising the train_list and val_list to the corresponding path"
      ],
      "metadata": {
        "id": "gM3coKeGp4g9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPmCG_Xecwcz"
      },
      "outputs": [],
      "source": [
        "train_list = np.load('/content/assessment_dataset/train_list.npy', allow_pickle=True).tolist()\n",
        "val_list = np.load('/content/assessment_dataset/val_list.npy', allow_pickle=True).tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#the below function will map the images and labels in the dataset\n"
      ],
      "metadata": {
        "id": "_GKv4H2RqJkc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spi0stZ_cwZw"
      },
      "outputs": [],
      "source": [
        "def read_xml_file(path):\n",
        "    \"\"\"\n",
        "    Read an XML file and extract relevant information.\n",
        "\n",
        "    Args:\n",
        "        path (str): Path to the XML file.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the extracted information.\n",
        "            - 'has_object' (bool): Indicates whether the XML file has an object.\n",
        "            - 'cat_or_dog' (str): Specifies the category of the object (cat or dog).\n",
        "            - 'xmin' (int): The x-coordinate of the top-left corner of the bounding box.\n",
        "            - 'ymin' (int): The y-coordinate of the top-left corner of the bounding box.\n",
        "            - 'xmax' (int): The x-coordinate of the bottom-right corner of the bounding box.\n",
        "            - 'ymax' (int): The y-coordinate of the bottom-right corner of the bounding box.\n",
        "            - 'specie' (str): The species specified in the XML file, extracted from the file path.\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If the XML file at the specified path does not exist.\n",
        "\n",
        "    \"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"File not found: {path}\")\n",
        "\n",
        "    with open(path, 'r') as f:\n",
        "        data = f.read()\n",
        "\n",
        "    bs_data = BeautifulSoup(data, 'xml')\n",
        "\n",
        "    return {\n",
        "        \"has_object\": True,\n",
        "        \"cat_or_dog\": bs_data.find(\"name\").text,\n",
        "        \"xmin\": int(bs_data.find(\"xmin\").text),\n",
        "        \"ymin\": int(bs_data.find(\"ymin\").text),\n",
        "        \"xmax\": int(bs_data.find(\"xmax\").text),\n",
        "        \"ymax\": int(bs_data.find(\"ymax\").text),\n",
        "        \"specie\": \"_\".join(path.split(os.sep)[-1].split(\"_\")[:-1])\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CUSTOM DATASET\n",
        "#Key in loading the dataset to training and validation\n",
        "#Here we are this function() loading dataset through this to the train() and val()"
      ],
      "metadata": {
        "id": "JuTE82ekqtdG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sr4cUmhGcwW0"
      },
      "outputs": [],
      "source": [
        "class CustomDataset():\n",
        "    \"\"\"\n",
        "    Custom dataset class for image classification and object detection.\n",
        "\n",
        "    Args:\n",
        "        dataset_path (str): The path to the dataset directory.\n",
        "        images_list (list): A list of image names.\n",
        "        train (bool, optional): Specifies whether the dataset is used for training or not. Default is False.\n",
        "\n",
        "    Attributes:\n",
        "        train (bool): Specifies whether the dataset is used for training or not.\n",
        "        dataset_path (str): The path to the dataset directory.\n",
        "        images_list (list): A list of image names.\n",
        "        image_folder_path (str): The path to the folder containing the images.\n",
        "        label_folder_path (str): The path to the folder containing the labels.\n",
        "        preprocess (torchvision.transforms.Compose): Preprocessing transformations to apply to the images.\n",
        "        data (list): A list of tuples containing the image path and label data.\n",
        "\n",
        "    Methods:\n",
        "        __len__(): Returns the length of the dataset.\n",
        "        __getitem__(index): Retrieves a specific image and its corresponding labels from the dataset.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset_path, images_list, train=False):\n",
        "        self.train = train\n",
        "        self.dataset_path = dataset_path\n",
        "        self.images_list = images_list\n",
        "        self.image_folder_path = os.path.join(dataset_path, \"images\")\n",
        "        self.label_folder_path = os.path.join(dataset_path, \"labels\")\n",
        "\n",
        "        self.preprocess = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(10),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "        self.data = []\n",
        "        for path in os.listdir(self.image_folder_path):\n",
        "            name = path.split(os.sep)[-1].split(\".\")[0]\n",
        "            if name in images_list:\n",
        "                image_path = os.path.join(self.image_folder_path, path)\n",
        "                xml_path = os.path.join(self.label_folder_path, name + \".xml\")\n",
        "                xml_data = read_xml_file(xml_path) if os.path.isfile(xml_path) else self.default_xml_data()\n",
        "                self.data.append((image_path, xml_data))\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the length of the dataset.\n",
        "\n",
        "        Returns:\n",
        "            int: The length of the dataset.\n",
        "\n",
        "        \"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Retrieves a specific image and its corresponding labels from the dataset.\n",
        "\n",
        "        Args:\n",
        "            index (int): The index of the image to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing the image and its corresponding labels.\n",
        "\n",
        "        \"\"\"\n",
        "        image_path, labels = self.data[index]\n",
        "        image = Image.open(image_path)\n",
        "        if self.preprocess is not None:\n",
        "            image = self.preprocess(image)\n",
        "\n",
        "        bbox = torch.tensor([labels[\"xmin\"], labels[\"ymin\"], labels[\"xmax\"], labels[\"ymax\"]])\n",
        "        has_object = torch.tensor(1 if labels[\"has_object\"] else 0, dtype=torch.float32)\n",
        "        cat_or_dog = torch.tensor(CAT_OR_DOG_TO_INDEX[labels[\"cat_or_dog\"]], dtype=torch.long)\n",
        "        specie = torch.tensor(SPECIES_TO_INDEX[labels[\"specie\"]], dtype=torch.long)\n",
        "\n",
        "        # Return as a dictionary\n",
        "        labels = {\"bbox\": bbox, \"has_object\": has_object, \"cat_or_dog\": cat_or_dog, \"specie\": specie}\n",
        "        return image, labels\n",
        "\n",
        "    def default_xml_data(self):\n",
        "        \"\"\"\n",
        "        Returns a default dictionary in case there is no object in the image.\n",
        "\n",
        "        Returns:\n",
        "            dict: The default label data dictionary.\n",
        "\n",
        "        \"\"\"\n",
        "        return {\n",
        "            \"has_object\": False,\n",
        "            \"cat_or_dog\": \"NA\",\n",
        "            \"specie\": \"NA\",\n",
        "            \"xmin\": 0,\n",
        "            \"ymin\": 0,\n",
        "            \"xmax\": 0,\n",
        "            \"ymax\": 0\n",
        "        }\n",
        "\n",
        "     "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MODEL\n",
        "#building the model as per our reqirement\n",
        "#results are based on this model"
      ],
      "metadata": {
        "id": "2w3eRVzkrLM7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhnGjMzrcwTJ"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    \"\"\"\n",
        "    Model class for the neural network.\n",
        "\n",
        "    Attributes:\n",
        "        backbone (nn.Sequential): The backbone architecture of the model.\n",
        "        avg_pool (nn.AdaptiveAvgPool2d): Adaptive average pooling layer.\n",
        "        dropout (nn.Dropout): Dropout layer for regularization.\n",
        "        have_object (nn.Linear): Fully connected layer for binary classification (object detection).\n",
        "        cat_or_dog (nn.Linear): Fully connected layer for multi-class classification (cat or dog).\n",
        "        specie (nn.Linear): Fully connected layer for multi-class classification (species).\n",
        "        bbox (nn.Linear): Fully connected layer for bounding box regression.\n",
        "\n",
        "    Methods:\n",
        "        forward(input): Performs a forward pass of the input through the model.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        #pretrained_model = resnet18(pretrained=True)\n",
        "        pretrained_model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
        "        self.backbone = nn.Sequential(*list(pretrained_model.children())[:-2])\n",
        "\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "        num_features = 2048\n",
        "\n",
        "        # Fully connected layers for each task\n",
        "        self.have_object = nn.Linear(num_features, 1)  # binary classification\n",
        "        self.cat_or_dog = nn.Linear(num_features, 3)  # multi-class classification\n",
        "        self.specie = nn.Linear(num_features, 10)  # multi-class classification\n",
        "        self.bbox = nn.Linear(num_features, 4)  # bounding box regression\n",
        "\n",
        "        # Xavier initialization\n",
        "        init.xavier_uniform_(self.have_object.weight)\n",
        "        init.xavier_uniform_(self.cat_or_dog.weight)\n",
        "        init.xavier_uniform_(self.specie.weight)\n",
        "        init.xavier_uniform_(self.bbox.weight)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Performs a forward pass of the input through the model.\n",
        "\n",
        "        Args:\n",
        "            input (torch.Tensor): The input tensor.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing the outputs of the model.\n",
        "\n",
        "        \"\"\"\n",
        "        out_backbone = self.backbone(input)\n",
        "        out_backbone = self.avg_pool(out_backbone)\n",
        "        out_backbone = self.dropout(out_backbone)\n",
        "        out_backbone = out_backbone.view(out_backbone.size(0), -1)  # Flatten\n",
        "\n",
        "        have_object = torch.sigmoid(self.have_object(out_backbone))\n",
        "        cat_or_dog = torch.softmax(self.cat_or_dog(out_backbone), dim=-1)\n",
        "        specie = torch.sigmoid(self.specie(out_backbone))\n",
        "        bbox = torch.sigmoid(self.bbox(out_backbone))\n",
        "\n",
        "        return {\n",
        "            \"bbox\": bbox,\n",
        "            \"object\": have_object,\n",
        "            \"cat_or_dog\": cat_or_dog,\n",
        "            \"specie\": specie\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Here we are defining some predefined paths so that we can alter the paths if we needed \n",
        "##MODEL_PATH to the alredy created dictonary\n",
        "##training BATCH_SIZE to 32\n",
        "##training dataset"
      ],
      "metadata": {
        "id": "WBCVggp1roOm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mAtlxfAcwQg"
      },
      "outputs": [],
      "source": [
        "MODEL_PATH = '/content/model'\n",
        "BATCH_SIZE = 32\n",
        "training_dataset = CustomDataset(\"/content/assessment_dataset\", images_list=train_list)\n",
        "training_loader = DataLoader(training_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "     "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training the Model and evaluating the model"
      ],
      "metadata": {
        "id": "rizGApsfr_wp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYkYkKfMcwLj"
      },
      "outputs": [],
      "source": [
        "def train(epochs, model_weights=None):\n",
        "    \"\"\"\n",
        "    Trains the model for the specified number of epochs.\n",
        "\n",
        "    Args:\n",
        "        epochs (int): The number of epochs to train the model.\n",
        "        model_weights (str): Path to the model weights file (default: None).\n",
        "\n",
        "    \"\"\"\n",
        "    # Initialize Model and Optimizer\n",
        "    model = Model()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Initialize Loss Functions\n",
        "    have_object_loss = torch.nn.BCEWithLogitsLoss()\n",
        "    specie_loss = torch.nn.CrossEntropyLoss()\n",
        "    cat_or_dog_loss = torch.nn.CrossEntropyLoss()\n",
        "    bbox_loss = torch.nn.SmoothL1Loss()\n",
        "\n",
        "    training_dataset = CustomDataset(\"/content/assessment_dataset\", images_list=train_list)\n",
        "    training_loader = DataLoader(training_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "    if model_weights is not None:\n",
        "        model.load_state_dict(torch.load(model_weights))\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "\n",
        "    best_val_metric = float('-inf')  # Variable to track the best validation metric score\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='max', patience=3, verbose=True)\n",
        "\n",
        "    def train_one_epoch(epoch_index):\n",
        "        \"\"\"\n",
        "        Trains the model for one epoch.\n",
        "\n",
        "        Args:\n",
        "            epoch_index (int): The index of the current epoch.\n",
        "\n",
        "        Returns:\n",
        "            float: The loss value for the last batch of the epoch.\n",
        "\n",
        "        \"\"\"\n",
        "        running_loss = 0.\n",
        "        last_loss = 0.\n",
        "\n",
        "        # Set model to training mode\n",
        "        model.train()\n",
        "\n",
        "        # Loop over each batch from the training set\n",
        "        for i, data in enumerate(training_loader):\n",
        "            # Unpack the inputs from our dataloader\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.cuda()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Compute loss\n",
        "            loss_have_object = have_object_loss(outputs[\"object\"].view(-1), labels[\"has_object\"].cuda())\n",
        "            loss_have_object = loss_have_object.sum()\n",
        "\n",
        "            loss_specie = specie_loss(outputs[\"specie\"], labels[\"specie\"].cuda())\n",
        "            loss_specie = loss_specie.sum()\n",
        "\n",
        "            loss_cat_or_dog = cat_or_dog_loss(outputs[\"cat_or_dog\"], labels[\"cat_or_dog\"].cuda())\n",
        "            loss_cat_or_dog = loss_cat_or_dog.sum()\n",
        "\n",
        "            loss_bbox = bbox_loss(outputs[\"bbox\"], labels[\"bbox\"].cuda())\n",
        "            loss_bbox = loss_bbox.sum()\n",
        "\n",
        "            # Consolidate losses\n",
        "            loss = loss_have_object + loss_specie + loss_cat_or_dog + loss_bbox\n",
        "\n",
        "            # Check if the result tensor requires gradient\n",
        "            if loss.requires_grad:\n",
        "                # Compute gradients and perform backpropagation\n",
        "                loss.backward()\n",
        "\n",
        "            # Perform optimization\n",
        "            optimizer.step()\n",
        "\n",
        "            # Gather data and report\n",
        "            running_loss += loss.item()\n",
        "            if i % 10 == 0:\n",
        "                last_loss = running_loss / 10  # loss per batch\n",
        "                running_loss = 0.\n",
        "        return last_loss\n",
        "\n",
        "    for i in range(epochs):\n",
        "        epoch_loss = train_one_epoch(i)\n",
        "        print(f' Epoch {i} Train Loss : {epoch_loss}')\n",
        "\n",
        "        metrics = validation(model, val_list)\n",
        "\n",
        "        val_object, val_cat_or_dog, val_specie, val_bbox = metrics\n",
        "\n",
        "        val_metric = val_object + val_cat_or_dog + val_specie + val_bbox\n",
        "\n",
        "        if val_metric > best_val_metric:\n",
        "            best_val_metric = val_metric\n",
        "            torch.save(model.state_dict(), f\"{MODEL_PATH}/best_model.pth\")\n",
        "\n",
        "        scheduler.step(best_val_metric)  # Adjust learning rate based on validation metric\n",
        "\n",
        "        print(metrics)\n",
        "\n",
        "\n",
        "def validation(model, val_list):\n",
        "    \"\"\"\n",
        "    Performs validation on the model.\n",
        "\n",
        "    Args:\n",
        "        model: The trained model.\n",
        "        val_list: List of validation images.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the validation scores for each metric.\n",
        "\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    val_dataset = CustomDataset(\"/content/assessment_dataset\", images_list=val_list)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "    metric_object = torchmetrics.Accuracy(task='binary').cuda()\n",
        "    metric_cat_or_dog = torchmetrics.Accuracy(task='multiclass', num_classes=3).cuda()\n",
        "    metric_specie = torchmetrics.Accuracy(task='multiclass', num_classes=10).cuda()\n",
        "    metric_bbox = torchmetrics.MeanAbsoluteError().cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(val_loader):\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.cuda()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            metric_object(outputs[\"object\"].view(-1), labels[\"has_object\"].cuda())\n",
        "            metric_cat_or_dog(outputs[\"cat_or_dog\"], labels[\"cat_or_dog\"].cuda())\n",
        "            metric_specie(outputs[\"specie\"], labels[\"specie\"].cuda())\n",
        "            metric_bbox(outputs[\"bbox\"], labels[\"bbox\"].cuda())\n",
        "\n",
        "    score_object = metric_object.compute()\n",
        "    score_cat_or_dog = metric_cat_or_dog.compute()\n",
        "    score_specie = metric_specie.compute()\n",
        "    score_bbox = metric_bbox.compute()\n",
        "\n",
        "    return score_object, score_cat_or_dog, score_specie, score_bbox\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#to check wheather CUDA is available or not\n"
      ],
      "metadata": {
        "id": "1BidLdNd0U_6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKG2inh95Edv",
        "outputId": "46fff29f-9244-4413-8e0f-fe2ddc59fb26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA version: 11.8\n",
            "GPU model: Tesla T4\n",
            "Number of GPUs: 1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU model: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
        "else:\n",
        "    print(\"CUDA is not available. Switch the runtime to GPU.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#training the data"
      ],
      "metadata": {
        "id": "Z_Teo14U0e97"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sXPfN0xcwIf",
        "outputId": "9fe3e606-707a-47f4-d7cd-bb260458ce57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 0 Train Loss : 147.04778900146485\n",
            "(tensor(0.9124, device='cuda:0'), tensor(0.7577, device='cuda:0'), tensor(0.4948, device='cuda:0'), tensor(143.3687, device='cuda:0'))\n",
            " Epoch 1 Train Loss : 152.17958221435546\n",
            "(tensor(0.7577, device='cuda:0'), tensor(0.6804, device='cuda:0'), tensor(0.4433, device='cuda:0'), tensor(143.4253, device='cuda:0'))\n",
            " Epoch 2 Train Loss : 155.99073944091796\n",
            "(tensor(0.9330, device='cuda:0'), tensor(0.8505, device='cuda:0'), tensor(0.5052, device='cuda:0'), tensor(143.3470, device='cuda:0'))\n",
            " Epoch 3 Train Loss : 147.98854217529296\n",
            "(tensor(0.8969, device='cuda:0'), tensor(0.7990, device='cuda:0'), tensor(0.4639, device='cuda:0'), tensor(143.3244, device='cuda:0'))\n",
            " Epoch 4 Train Loss : 142.98466873168945\n",
            "(tensor(0.9588, device='cuda:0'), tensor(0.8918, device='cuda:0'), tensor(0.4691, device='cuda:0'), tensor(143.3255, device='cuda:0'))\n",
            " Epoch 5 Train Loss : 136.53680572509765\n",
            "(tensor(0.9124, device='cuda:0'), tensor(0.7165, device='cuda:0'), tensor(0.4381, device='cuda:0'), tensor(143.3662, device='cuda:0'))\n",
            " Epoch 6 Train Loss : 146.20548400878906\n",
            "Epoch 00007: reducing learning rate of group 0 to 1.0000e-04.\n",
            "(tensor(0.9588, device='cuda:0'), tensor(0.8299, device='cuda:0'), tensor(0.3918, device='cuda:0'), tensor(143.3048, device='cuda:0'))\n",
            " Epoch 7 Train Loss : 144.39596252441407\n",
            "(tensor(0.9691, device='cuda:0'), tensor(0.9175, device='cuda:0'), tensor(0.6701, device='cuda:0'), tensor(143.3019, device='cuda:0'))\n",
            " Epoch 8 Train Loss : 151.02035217285157\n",
            "(tensor(0.9588, device='cuda:0'), tensor(0.9227, device='cuda:0'), tensor(0.6856, device='cuda:0'), tensor(143.3034, device='cuda:0'))\n",
            " Epoch 9 Train Loss : 147.58859939575194\n",
            "(tensor(0.9639, device='cuda:0'), tensor(0.9485, device='cuda:0'), tensor(0.6649, device='cuda:0'), tensor(143.3044, device='cuda:0'))\n",
            " Epoch 10 Train Loss : 145.12516174316406\n",
            "(tensor(0.9742, device='cuda:0'), tensor(0.9485, device='cuda:0'), tensor(0.7629, device='cuda:0'), tensor(143.2980, device='cuda:0'))\n",
            " Epoch 11 Train Loss : 144.95886459350587\n",
            "(tensor(0.9742, device='cuda:0'), tensor(0.9433, device='cuda:0'), tensor(0.7784, device='cuda:0'), tensor(143.2918, device='cuda:0'))\n",
            " Epoch 12 Train Loss : 149.50909042358398\n",
            "(tensor(0.9691, device='cuda:0'), tensor(0.9639, device='cuda:0'), tensor(0.7680, device='cuda:0'), tensor(143.2924, device='cuda:0'))\n",
            " Epoch 13 Train Loss : 141.76985397338868\n",
            "(tensor(0.9742, device='cuda:0'), tensor(0.9433, device='cuda:0'), tensor(0.8041, device='cuda:0'), tensor(143.2947, device='cuda:0'))\n",
            " Epoch 14 Train Loss : 147.28574752807617\n",
            "(tensor(0.9742, device='cuda:0'), tensor(0.9485, device='cuda:0'), tensor(0.7732, device='cuda:0'), tensor(143.2905, device='cuda:0'))\n",
            " Epoch 15 Train Loss : 145.26000366210937\n",
            "(tensor(0.9691, device='cuda:0'), tensor(0.9536, device='cuda:0'), tensor(0.7990, device='cuda:0'), tensor(143.2921, device='cuda:0'))\n",
            " Epoch 16 Train Loss : 149.3875503540039\n",
            "(tensor(0.9794, device='cuda:0'), tensor(0.9588, device='cuda:0'), tensor(0.7938, device='cuda:0'), tensor(143.2878, device='cuda:0'))\n",
            " Epoch 17 Train Loss : 139.03948516845702\n",
            "Epoch 00018: reducing learning rate of group 0 to 1.0000e-05.\n",
            "(tensor(0.9691, device='cuda:0'), tensor(0.9536, device='cuda:0'), tensor(0.7938, device='cuda:0'), tensor(143.2935, device='cuda:0'))\n",
            " Epoch 18 Train Loss : 151.58880920410155\n",
            "(tensor(0.9742, device='cuda:0'), tensor(0.9536, device='cuda:0'), tensor(0.8041, device='cuda:0'), tensor(143.2865, device='cuda:0'))\n",
            " Epoch 19 Train Loss : 153.2283493041992\n",
            "(tensor(0.9639, device='cuda:0'), tensor(0.9485, device='cuda:0'), tensor(0.7938, device='cuda:0'), tensor(143.2946, device='cuda:0'))\n"
          ]
        }
      ],
      "source": [
        "train(20, model_weights=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#randomly selecting the images like 50\n",
        "##after training, we have to take some images and visualize our results\n",
        "##to visualize, iam taking radmolyly, so that we can know the capability of our model"
      ],
      "metadata": {
        "id": "AJIel6Qk05Oq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1ALuH2icwBt"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import shutil\n",
        "\n",
        "def copy_random_files(source_folder, destination_folder, num_files):\n",
        "    file_list = os.listdir(source_folder)\n",
        "    random_files = random.sample(file_list, num_files)\n",
        "    if not os.path.exists(destination_folder):\n",
        "      os.makedirs(destination_folder)\n",
        "    \n",
        "    for file_name in random_files:\n",
        "        source_path = os.path.join(source_folder, file_name)\n",
        "        destination_path = os.path.join(destination_folder, file_name)\n",
        "        shutil.copy2(source_path, destination_path)\n",
        "\n",
        "# usage:\n",
        "source_folder = 'assessment_dataset/images'\n",
        "destination_folder = '/content/testImgs'\n",
        "num_files_to_copy = 50\n",
        "\n",
        "copy_random_files(source_folder, destination_folder, num_files_to_copy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#There is a problem while using the data from testimgs folder\n",
        "##that folder is in .jpeg and .jpeg folder\n",
        "##while iam trying to put the name of the file in visualize(), it is showing error\n",
        "##so i created a function and list to get the file name without the extension "
      ],
      "metadata": {
        "id": "yaO7V6BZ1vF_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zM-Fb7wmcu0t",
        "outputId": "2154325c-6cd0-4436-d9d5-5cdc4750f057"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['basset_hound_190', 'pomeranian_150', 'chihuahua_150', 'american_pit_bull_terrier_128', 'beagle_113', 'basset_hound_182', 'Abyssinian_130', 'beagle_150', 'Persian_164', 'chihuahua_173', 'pomeranian_126', 'Birman_112', 'Abyssinian_135', 'Persian_141', 'beagle_173', 'n02006656_spoonbill', 'basset_hound_137', 'beagle_163', 'n01807496_partridge', 'american_pit_bull_terrier_112', 'chihuahua_167', 'pomeranian_109', 'chihuahua_169', 'basset_hound_186', 'pomeranian_111', 'american_bulldog_185', 'basset_hound_154', 'Birman_177', 'Persian_201', 'pomeranian_123', 'american_pit_bull_terrier_100', 'pomeranian_182', 'chihuahua_120', 'n01675722_banded_gecko', 'Abyssinian_175', 'pomeranian_114', 'Abyssinian_137', 'american_bulldog_112', 'Persian_195', 'Abyssinian_126', 'basset_hound_122', 'american_bulldog_18', 'american_bulldog_134', 'basset_hound_132', 'beagle_162', 'basset_hound_156', 'pomeranian_172', 'american_bulldog_132', 'beagle_117', 'beagle_146', 'american_bulldog_11', 'Persian_136', 'chihuahua_177', 'Birman_140', 'american_bulldog_201', 'beagle_116', 'chihuahua_17', 'Birman_121', 'chihuahua_122', 'pomeranian_169', 'Abyssinian_141', 'Abyssinian_11', 'Persian_183', 'n02009229_little_blue_heron', 'Persian_158', 'beagle_178', 'basset_hound_116', '00055', 'american_bulldog_135', 'Birman_163', 'Persian_138', 'american_bulldog_13', 'basset_hound_161', '00017', 'chihuahua_181', 'Persian_149', 'Abyssinian_123', 'american_pit_bull_terrier_146', 'chihuahua_187', 'n01775062_wolf_spider', 'american_bulldog_123', '4', 'Abyssinian_122', 'Persian_118', 'basset_hound_112', 'american_bulldog_131', 'pomeranian_137', 'pomeranian_133', 'Persian_160', 'american_bulldog_129', 'american_pit_bull_terrier_163', 'Persian_170', 'Abyssinian_16', 'pomeranian_124', 'basset_hound_179', 'Abyssinian_139', 'chihuahua_100', 'basset_hound_153', 'chihuahua_140', 'beagle_153', 'n01968897_chambered_nautilus', 'Birman_181', 'american_pit_bull_terrier_176', 'chihuahua_152', 'basset_hound_133', 'n01770393_scorpion']\n"
          ]
        }
      ],
      "source": [
        "def get_image_names(image_folder):\n",
        "    image_names = []\n",
        "    for filename in os.listdir(image_folder):\n",
        "        if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\"):\n",
        "            name = os.path.splitext(filename)[0]\n",
        "            image_names.append(name)\n",
        "    return image_names\n",
        "\n",
        "# Example usage:\n",
        "folder_path = \"/content/testImgs\"\n",
        "image_names = get_image_names(folder_path)\n",
        "print(image_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#to visualize iam taking the help of yolo "
      ],
      "metadata": {
        "id": "tx6Rca2F2su2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvZpOJmFiXav",
        "outputId": "3073f91f-8450-4920-c81c-69644c4325b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: imageai in /usr/local/lib/python3.10/dist-packages (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install imageai #pillow>=7.0.0 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTFehfkmiXZF"
      },
      "outputs": [],
      "source": [
        "from imageai.Detection import ObjectDetection\n",
        "\n",
        "detector = ObjectDetection()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6P8fhc4iXTI"
      },
      "outputs": [],
      "source": [
        "detector.setModelTypeAsYOLOv3()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9PzYyJMiXP1",
        "outputId": "c5037d62-3939-40b8-b288-f93288b03f8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-22 09:11:32--  https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/yolov3.pt\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/125932201/adc7efe4-b3ac-4710-8a05-0bfefa255bae?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230522%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230522T091132Z&X-Amz-Expires=300&X-Amz-Signature=17de4910280f6efd955fb8396dc333ecca0fe8095ea587251efdeb0ad770278b&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=125932201&response-content-disposition=attachment%3B%20filename%3Dyolov3.pt&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-05-22 09:11:32--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/125932201/adc7efe4-b3ac-4710-8a05-0bfefa255bae?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230522%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230522T091132Z&X-Amz-Expires=300&X-Amz-Signature=17de4910280f6efd955fb8396dc333ecca0fe8095ea587251efdeb0ad770278b&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=125932201&response-content-disposition=attachment%3B%20filename%3Dyolov3.pt&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248148565 (237M) [application/octet-stream]\n",
            "Saving to: ‘yolov3.pt.1’\n",
            "\n",
            "yolov3.pt.1         100%[===================>] 236.65M   159MB/s    in 1.5s    \n",
            "\n",
            "2023-05-22 09:11:33 (159 MB/s) - ‘yolov3.pt.1’ saved [248148565/248148565]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/OlafenwaMoses/ImageAI/releases/download/3.0.0-pretrained/yolov3.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#setting the yolo model"
      ],
      "metadata": {
        "id": "AuHAIvAU26yX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLBjvDGTiXNU"
      },
      "outputs": [],
      "source": [
        "detector.setModelTypeAsYOLOv3()\n",
        "detector.setModelPath(\"/content/yolov3.pt\")\n",
        "#detector.setJsonPath(\"hololens-yolo_yolov3_detection_config.json\")\n",
        "detector.loadModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#defining a function to draw the image with the help of yolo"
      ],
      "metadata": {
        "id": "g3AeG8cl2_il"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l22YxoqiiXGB"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def image_show(image_path) :\n",
        "  # Load image\n",
        "  #image_path = image_s\n",
        "   #image_s = os.path.join(image_folder_path, image_name + \".jpeg\")\n",
        "\n",
        "  annotations = detector.detectObjectsFromImage(input_image=image_path, minimum_percentage_probability=30)\n",
        "  imag = Image.open(image_path)\n",
        "\n",
        "  # Get annotation data\n",
        "  #annotations = [{'name': 'car', 'percentage_probability': 99.98, 'box_points': [20, 51, 252, 145]}]\n",
        "\n",
        "  # Draw bounding box and label on image\n",
        "  draw = ImageDraw.Draw(imag)\n",
        "  #font = ImageFont.truetype(\"arial.ttf\", 16)\n",
        "  for annotation in annotations:\n",
        "\n",
        "    name = annotation['name']\n",
        "    box = annotation['box_points']\n",
        "    draw.rectangle(box, outline='red', width=2)\n",
        "    draw.text((box[0], box[1] - 20), name, fill='red')\n",
        "\n",
        "  # Display image\n",
        "  plt.imshow(imag)\n",
        "  plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#predefing the variables, so that we can get the custom output"
      ],
      "metadata": {
        "id": "WNklBmy83IO0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWAC0NRvcvQL"
      },
      "outputs": [],
      "source": [
        "OUTPUT_PATH = '/content/output'\n",
        "MODEL_WEIGHTS = f\"{MODEL_PATH}/best_model.pth\"\n",
        "image_folder_path = '/content/testImgs'\n",
        "images = image_names"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#VISUALIZATION"
      ],
      "metadata": {
        "id": "izISbSZt3RXJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LPCmKmicvOh"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def visualize(model_weights, image_names, image_folder_path, output_folder=\"output\"):\n",
        "    \"\"\"\n",
        "    Visualizes the predictions of the model on the specified images.\n",
        "\n",
        "    Args:\n",
        "        model_weights (str): Path to the model weights file.\n",
        "        image_names (list): List of image names to visualize.\n",
        "        image_folder_path (str): Path to the folder containing the images.\n",
        "        output_folder (str): Path to the output folder (default: \"output\").\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the visualization results for each image.\n",
        "\n",
        "    \"\"\"\n",
        "    model = Model()\n",
        "    model.load_state_dict(torch.load(model_weights, map_location=torch.device('cpu')))\n",
        "    model.eval()\n",
        "    results = {}\n",
        "\n",
        "    for image_name in image_names:\n",
        "        try:\n",
        "            image = Image.open(os.path.join(image_folder_path, image_name + \".jpg\"))\n",
        "            image_s = os.path.join(image_folder_path, image_name + \".jpg\")\n",
        "        except:\n",
        "            image = Image.open(os.path.join(image_folder_path, image_name + \".jpeg\"))\n",
        "            image_s = os.path.join(image_folder_path, image_name + \".jpeg\")\n",
        "\n",
        "\n",
        "        # Preprocessing\n",
        "        preprocess = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "        input_tensor = preprocess(image)\n",
        "        input_batch = input_tensor.unsqueeze(0)\n",
        "\n",
        "        input_batch = input_batch.to('cpu')  # Move input tensor to CPU\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(input_batch)\n",
        "\n",
        "        bbox = output['bbox'].cpu().numpy()\n",
        "        bbox = bbox.squeeze()  # Remove any extra dimensions\n",
        "        bbox = bbox.tolist()  # Convert to a list\n",
        "\n",
        "        have_object = output['object'].item() > 0.5\n",
        "\n",
        "        cat_or_dog_index = output['cat_or_dog'].argmax(dim=1).item()\n",
        "        cat_or_dog = next(key for key, value in CAT_OR_DOG_TO_INDEX.items() if value == cat_or_dog_index)\n",
        "\n",
        "        species_index = output['specie'].argmax(dim=1).item()\n",
        "        species_name = next(key for key, value in SPECIES_TO_INDEX.items() if value == species_index)\n",
        "\n",
        "        #fig, ax = plt.subplots(1)\n",
        "        #ax.imshow(image)\n",
        "\n",
        "        image_name_ = image_s\n",
        "\n",
        "        values = {\n",
        "            'has_object': have_object,\n",
        "            'cat_or_dog': cat_or_dog,\n",
        "            'specie': species_name,\n",
        "            'xmin': bbox[0],\n",
        "            'ymin': bbox[1],\n",
        "            'xmax': bbox[2],\n",
        "            'ymax': bbox[3]\n",
        "        }\n",
        "        results[image_name_] = values\n",
        "        #print(f\"{image_name}, :\" image_name)\n",
        "        print(f\"IMAGE NAME : {image_name}, \\nIMAGE PATH: {image_name_}\")\n",
        "        image_show(image_s)\n",
        "        print(f\"RESULTS OF THE IMAGE:{image_name}\")\n",
        "        print(results[image_name_])\n",
        "\n",
        "        print(\"*******************************************************************************************************\")\n",
        "        print(\"NEXT IMAGE\")\n",
        "\n",
        "    return "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##running the visualize function."
      ],
      "metadata": {
        "id": "g5a3xJVf3X2_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cvruKWPPiXCb",
        "outputId": "b5d954d0-ddb3-4a9a-ec7c-873cff96a62b"
      },
      "outputs": [],
      "source": [
        "output = visualize(MODEL_WEIGHTS, images, image_folder_path, output_folder=OUTPUT_PATH)\n",
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtvBsiS5iW7x"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}